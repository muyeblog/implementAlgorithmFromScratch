{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbL1V8UvHiCNiYcFhpq8a5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muyeblog/implementAlgorithmFromScratch/blob/main/LogisticRegression_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries for logistic Regression"
      ],
      "metadata": {
        "id": "B3-Wo2uvSvp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "参看：https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/"
      ],
      "metadata": {
        "id": "N6Gzxm6LW7SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import log,dot,e,shape\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TBfMn7b8SrZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import dataset"
      ],
      "metadata": {
        "id": "wDSL4zq_TAeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X,y = make_classification(n_features=4,n_classes=2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)"
      ],
      "metadata": {
        "id": "2Y3PW-W2S_Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardization\n",
        "Standardization is the process of scaling data around the mean with a unit standard deviation(使用单位标准差围绕均值缩放数据).That means we are effectively making the mean of the attribute zero with the resulting distribution having a standard deviation equal to zero(这意味着我们有效的使属性的平均值为 0，标准差分布为 0).Some algorithms are vulnerable to features with different scales.Especially if we are using gradient descent for optimization, then the model will have a hard time giving accurate results; for example,if a dataset has two features, age and salary, then the salary feature with its higher range will most likely dominate(主导) the outcome. So,it is a good practice to standardize data before feeding it to the algorithm. Highly recommended to go through this article to understand standardization clearly. Mathematically it it given as \n",
        "\n",
        "(x-μ)/σ\n",
        "\n",
        "This is often necessary when attributes are from different scales."
      ],
      "metadata": {
        "id": "f82FJpjVTbY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(X_tr):\n",
        "  for i in range(shape(X_tr)[1]):\n",
        "    X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])"
      ],
      "metadata": {
        "id": "11svVR57TaNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Parameters\n",
        "\n",
        "The data sets are always multidimensional. We will need to use matrics for any kind of calculation. So,for input,we have two matrices to deal with.The first one is for feature vectors, and the second is for parameters or weights. Our first matrix is of the m x n dimension, where m is the number of observations while n is the dimension of observations. And the second on is of n x 1 dimension. Here, we will add a bias column of ones to our feature vectors matrix and a corresponding parameter term to weight vector. Bias is import to make the model more flexible."
      ],
      "metadata": {
        "id": "4xo9ln3bX8vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize(self,X):\n",
        "  weights = np.zeros((shape(X)[1]+1,1))\n",
        "  X = np.c_[np.ones((shape(X)[0],1)),X]\n",
        "  return weights,X"
      ],
      "metadata": {
        "id": "2Zwcx3EiX7FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo:\n",
        "demo = np.array([[1,2,3],[4,5,6]])\n",
        "print(shape(demo))\n",
        "print(np.ones((shape(demo)[0],1))) # 创建了与样本数量一致的bias为 1 的数组\n",
        "print(np.c_[np.ones((shape(demo)[0],1)),demo]) # 将 bias数组与样本数组拼接"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyC36jpOZ3Cd",
        "outputId": "e723d3c3-8716-4765-c981-19bce090fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "[[1.]\n",
            " [1.]]\n",
            "[[1. 1. 2. 3.]\n",
            " [1. 4. 5. 6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.c_[demo,np.ones((shape(demo)[0],1))]\n",
        "# np.c_是按行连接两个矩阵，就是把两矩阵左右相拼接，要求行数相等。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxa2ojT2YVsB",
        "outputId": "45a0c458-7f87-41d2-ed88-a47273a206d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3., 1.],\n",
              "       [4., 5., 6., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In the code above, although we have initialized the weight vector to be a vector of zeros, you could opt for any other values as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "5PVQCNole8jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Function\n",
        "In a linear regression model,the hypothesis function is a linear combination of parameters given as y=ax+b for a simple single parameter data. This allows us to predict continuous values effectively,but in logistic regression, the response variables are binomial, either 'yes' or 'no'. So, it makes less sense to use the linear function to predict anything except the values between 0 and 1. And the most effective function to limit results of a linear equation to  [0,1] is the sigmoid or logistic function.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/22366480px-Logistic-curve.svg.png)\n",
        "\n",
        "As you can see , the sigmoid function intersects the y-axis at 0.5. In most cases, we  use this point as a threshold for classification. Any value above it will be classified as 1, while any value below is 0. This is not a rule of thumb(这不是经验法则). We can also use different values instead of 0.5 , depending on the requirements. The sigmoid function:\n",
        "\n",
        "$y = 1 / (1 + e^{-x})$\n",
        "\n",
        "We plug the linear equation in  place of x.\n",
        "\n",
        "$y = 1/(1 + e^{-(ax+b)})$"
      ],
      "metadata": {
        "id": "OPjCSbAdfNL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(self,z):\n",
        "  sig = 1/(1 + e**(-z))\n",
        "  return sig"
      ],
      "metadata": {
        "id": "CRCLgVLGZ5P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above expression, z is the dot product of m x n matrix containing observation and n x 1 matrix of weights."
      ],
      "metadata": {
        "id": "puxo7_guh678"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Function\n",
        "Cost function or loss function is that function that describes how much the calculated value deviates from the actual value. Linear regression employs the least squared error as the cost function. But the least squared error function for logistic regression is non-convex(非凸的). While performing gradient descent chances that we get stuck in a local minimum is more. So instead, we use log loss as the cost function.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/67304NN_C4.png)"
      ],
      "metadata": {
        "id": "Biecys5CiN48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula gives the cost function for the logistic regression.\n",
        "\n",
        "$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_{\\theta}(x^{(i)})) +(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)})]$\n",
        "\n",
        "where $h(x)$ is the sigmoid function we used earlier."
      ],
      "metadata": {
        "id": "8sMbUqn5jLil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(self,theta):\n",
        "  z = dot(X,theta)\n",
        "  cost0 = y.T.dot(log(self.sigmoid(z)))\n",
        "  cost1  = (1-y).T.dot(log(1-self.sigmoid(z)))\n",
        "  cost = -((cost1 + cost0))/len(y)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "4Kx8SAbAh51Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "The next step is gradient descent. Gradient descent is an optimization algorithm that is responsible for the learning of best-fitting parameters. So what are the gradients? The gradients are the vector of the 1st order derivative of the cost function.These are the direction of the steepest ascent or maximum of a function. For gradient descent,we move in the opposite direction of the gradients. We will bee updating the weights in every iteration until the convergence. The pseudocode for Gradient descent looks something like this.\n",
        "\n",
        "![](图裂)\n",
        "\n",
        "Here,the alpha is the step size responsible for how quick it converages to the global minimum. If the step size is too small, it will converage slowly, but if it is too large, it may overshoot the minimum while descending.\n",
        "\n",
        "By differentiating the cost function, we get the gradient descent expression\n",
        "\n",
        "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{2m}\\sum_{i=1}^m (h(x^{(i)})-y)x_j^{(i)}$\n",
        "\n",
        "or\n",
        "\n",
        "$X^T(h_{\\theta}(x) - y)$\n",
        "\n",
        "This is vectorised form of the gradient expression, which we will be using in our code."
      ],
      "metadata": {
        "id": "5qfKDpFLcP8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self,X,y,alpha=0.001,iter=100):\n",
        "  params,X = self.initialize(X)\n",
        "  cost_list = np.zeros(iter,)\n",
        "  for i in range(iter):\n",
        "    params = params - alpha * dot(X.T,self.sigmoid(dot(X,params)) - np.reshape(y,(len(y),1)))\n",
        "    cost_list[i] = cost(params)\n",
        "  self.params = params\n",
        "  return cost_list\n"
      ],
      "metadata": {
        "id": "fUnZYOv_k3Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction \n",
        "Everything that we have done far is for this step. We trained the model on a training dataset, and we will use the learned parameters to predict the unseen data."
      ],
      "metadata": {
        "id": "PzhpAXqZgUjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self,X):\n",
        "  z = dot(self,initialize(X)[1],self.weights)\n",
        "  lis = []\n",
        "  for i in self.sigmoid(z):\n",
        "    if i > 0.5:\n",
        "      lis.append(1)\n",
        "    else:\n",
        "      lis.append(0)\n",
        "  return lis"
      ],
      "metadata": {
        "id": "MvzW8XTlgTpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1-Score\n",
        "Now that we are done with the prediction, we will move on to the F1-score section, where we will measure how good our model predicts for unseen data. The F1-score is a robust metric for evaluating the performances of classification modelss, and mathematically F1-score is the harmonic mean of precision and recall(从数学上来讲，F1-score是准确率和召回率的调和平均值).\n",
        "\n",
        "Recall, Precision,F1 Score - Simple Metric Explanation Machine Learning\n",
        "\n",
        "precision = Precision is the number of true positives over the sum of true positives and false positives.\n",
        "precision = TP/(TP + FP)\n",
        "\n",
        "recall =  Recall is the number of true positives over  the sum of true postive positives and false negatives.\n",
        "recall = TP/(TP + FN)"
      ],
      "metadata": {
        "id": "z9FCq77ag70v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score(y,y_hat):\n",
        "  tp,tn,fp,fn=0,0,0,0\n",
        "  for i in range(len(y)):\n",
        "    if y[i] == 1 and y_hat[i]==1:\n",
        "      tp += 1\n",
        "    elif y[i]==1  and y_hat[i]==0:\n",
        "      fn += 1\n",
        "    elif y[i]==0 and  y_hat[i]==1:\n",
        "      fp += 1\n",
        "    elif y[i]==0 and y_hat[i]==0:\n",
        "      tn += 1\n",
        "  precision = tp /(tp + fp)\n",
        "  recall = tp/(tp + fn)\n",
        "  f1_score = 2*precision*recall/(precision + recall)\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "C5VcJP5tg6lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting Everything Together: Logistic Regression\n",
        "\n",
        "Now that we are done with  every part, we will put everything together in a single class."
      ],
      "metadata": {
        "id": "Zrs-vNqQjSVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import log,dot,exp,shape\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X,y = make_classification(n_features=4)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)"
      ],
      "metadata": {
        "id": "8BC7UY1pjRvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import log,dot,exp,shape\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X,y = make_classification(n_features=4)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
        "\n",
        "\n",
        "def standardize(X_tr):\n",
        "  for i in range(shape(X_tr)[1]):\n",
        "    X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])\n",
        "\n",
        "def F1_score(y,y_hat):\n",
        "  tp,tn,fp,fn = 0,0,0,0\n",
        "  for i in range(len(y)):\n",
        "    if y[i] == 1 and y_hat[i] == 1:\n",
        "      tp += 1\n",
        "    elif y[i] == 1 and y_hat[i] == 0:\n",
        "      fn += 1\n",
        "    elif y[i] == 0 and y_hat[i] == 1:\n",
        "      fp += 1\n",
        "    elif y[i] == 0 and y_hat[i] == 0:\n",
        "      tn += 1\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f1_score = 2*precision*recall / (precision + recall)\n",
        "  return  f1_score\n",
        "\n",
        "class LogisticRegression:\n",
        "  def sigmoid(self,z):\n",
        "    sig = 1 / (1 + exp(-z))\n",
        "    return sig\n",
        "  \n",
        "  def initialize(self,X):\n",
        "    weights = np.zeros((shape(X)[1]+1,1))\n",
        "    X = np.c_[np.ones((shape(X)[0],1)),X] # 添加并拼接上bias系数列\n",
        "    return  weights,X\n",
        "  \n",
        "  def fit(self,X,y,alpha=0.01,iter=400):\n",
        "    weights,X = self.initialize(X)\n",
        "    def cost(theta):\n",
        "      # dot 为矩阵乘法\n",
        "      z = dot(X,theta) # 一个样本一个z，z的 shape 为 (sample_size,1)\n",
        "      # y shape (sample_size,1) y.T shape (1,sample_size)\n",
        "      cost0 = y.T.dot(log(self.sigmoid(z))) \n",
        "      cost1 = (1-y).T.dot(log(1-self.sigmoid(z)))\n",
        "      # print('cost0',cost0)\n",
        "      cost = -(cost0 + cost1)/len(y)\n",
        "      # print('cost',cost)\n",
        "      # print(cost)\n",
        "      return cost\n",
        "    cost_list = np.zeros(iter,)\n",
        "    for i in range(iter):\n",
        "      weights = weights - alpha*dot(X.T,self.sigmoid(dot(X,weights)) - np.reshape(y,(len(y),1)))\n",
        "      cost_list[i] = cost(weights)\n",
        "    self.weights = weights\n",
        "    return cost_list\n",
        "  \n",
        "  def predict(self,X):\n",
        "    z = dot(self.initialize(X)[1],self.weights)\n",
        "    lis = []\n",
        "    for i in self.sigmoid(z):\n",
        "      if i > 0.5:\n",
        "        lis.append(1)\n",
        "      else:\n",
        "        lis.append(0)\n",
        "    return lis"
      ],
      "metadata": {
        "id": "Wb93a0gUjQXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_v9VnD-dk0q",
        "outputId": "dd48e057-7484-49ef-ca07-8e8a1da9caf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr[:3,]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sltT82_XdliV",
        "outputId": "b10d4f69-a776-421c-9ba0-3811eac28f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.44733753,  1.46761239, -0.65926372, -1.06834666],\n",
              "       [ 2.02239535, -0.04916234, -1.0227897 ,  1.08891915],\n",
              "       [-0.31281433,  0.08904525,  0.10869519, -0.21469059]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standardize(X_tr)\n",
        "standardize(X_te)\n",
        "obj1 = LogisticRegression()\n",
        "model = obj1.fit(X_tr,y_tr)"
      ],
      "metadata": {
        "id": "Y7lzCmrEdrEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = obj1.predict(X_te)\n",
        "y_train = obj1.predict(X_tr)"
      ],
      "metadata": {
        "id": "m77BLDbod9Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's see the f1-score for training and testing\n",
        "f1_score_tr = F1_score(y_tr,y_train)\n",
        "f1_score_te = F1_score(y_te,y_pred)\n",
        "print(f1_score_tr)\n",
        "print(f1_score_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3fhIBve2IE",
        "outputId": "bbe0c695-8ce1-4681-dfa1-8fdc715bf0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9555555555555556\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,let's see how our logistic regression fares in comparison to sklearn's logistic regression"
      ],
      "metadata": {
        "id": "zNeSFFuolbSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "model = LogisticRegression().fit(X_tr,y_tr)\n",
        "y_pred = model.predict(X_te)\n",
        "print(f1_score(y_te,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbwhhKrdlaRg",
        "outputId": "cbda0924-0678-4e9c-f330-2da93df469c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  sig = 1/(1 + exp(-z))\n",
        "  return sig"
      ],
      "metadata": {
        "id": "ySYAm-1shdVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr.T.dot(log(sigmoid(X_tr)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OXzFLXzhgNK",
        "outputId": "0d78a0b2-f709-4cfe-d164-b0ebf3ef1dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-17.84327904, -26.90887447, -54.09310159, -37.03621028])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(np.array([[1,2,3]]),np.array([[1,2,3]]).T).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQiZRK9nhV01",
        "outputId": "9612d104-f4b5-4e38-9076-ad0bcaefaeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[1,2,3]]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xkfXsTgkaWu",
        "outputId": "26a0f1e5-2e08-4809-b3ac-e424a63ba890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[1,2,3]]).T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usd5-oRhkirt",
        "outputId": "0ce5d78f-cc6d-47b9-a52c-75961883acd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[1,2,3]]).dot(np.array([[1,2,3]]).T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lVYgJ8WlC2c",
        "outputId": "9443d5a2-d694-477e-e491-6ab160d0c422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(np.array([[1,2,3]]),np.array([[1,2,3]]).T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLJSJlYhlHJy",
        "outputId": "bca12622-9b2e-4c65-a30b-840f3d6d1490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log(np.array([[1,2,3]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfqme4N4iqJy",
        "outputId": "2af1dea1-17ba-4f81-a88c-6af4819e39f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.69314718, 1.09861229]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr[:5,]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VORwL3TwkTDY",
        "outputId": "51233a4b-d71d-49b2-f1d7-84687afa50f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.20449986,  0.96212659, -0.60551239, -0.79812379],\n",
              "       [ 1.18580724,  0.06441484, -0.88098499,  0.74745564],\n",
              "       [-0.12877161,  0.14621379, -0.02356874, -0.18651939],\n",
              "       [ 0.09042609, -1.94393966,  1.44955939,  1.39707681],\n",
              "       [-0.57298926,  0.16115541,  0.27604995, -0.49339226]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dot 计算矩阵乘法时：\n",
        "设A是$n\\times m$的矩阵，B是$m\\times p$的矩阵，则它们的矩阵积AB是$n\\times p$的矩阵.\n",
        "\n",
        "dot 计算向量内积时：\n",
        "向量是一维矩阵，两个向量进行内积运算时，需要保证两个向量包含的元素个数是相同的"
      ],
      "metadata": {
        "id": "bka6n-hmno3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1,2,3,4,5,6,7])\n",
        "print(x.shape)  # (7,) 为一维矩阵，即向量的维度\n",
        "print(x.T.shape)  # 转置前后维度不变 (7,) 为一维矩阵，即向量的维度\n",
        "y = np.array([2,3,4,5,6,7,8])\n",
        "result = np.dot(x, y) \n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNZAz4WRk56y",
        "outputId": "1b165a8f-5770-4b28-a8fd-c1eeae668b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7,)\n",
            "(7,)\n",
            "168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zHrRumKKnqi1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}