{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6obwjSQ96soiSSglS78zM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muyeblog/implementAlgorithmFromScratch/blob/master/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kej93otBp15i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from abc import ABCMeta, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class loss(metaclass=ABCMeta):\n",
        "  '''\n",
        "  The abstract base class for loss function.\n",
        "  For each loss, the gradient should be specified.\n",
        "  '''\n",
        "  def obj(self,pred,true):\n",
        "    pass\n",
        "  \n",
        "  def gradient(self,pred,true):\n",
        "    pass\n",
        "  \n",
        "\n",
        "class mse(loss):\n",
        "  '''Loss function for mse.'''\n",
        "  def obj(self,pred,true):\n",
        "    return np.square(pred-true).mean()/2\n",
        "  \n",
        "  def gradient(self,pred,true): # 表示对其中一个pred求导\n",
        "    return pred-true\n",
        "\n",
        "class log_loss(Loss):\n",
        "  '''Loss function for log loss'''\n",
        "  def obj(self,pred,true):\n",
        "    return (-np.multiply(true,np.log(pred)) - np.multiply(1-true,np.log(1-pred))).mean()\n",
        "  \n",
        "  def gradient(self,pred,true):\n",
        "     # 这里是对 pred求导(对其中一个pred 求导)\n",
        "     return -np.multiply(true,1/pred)+np.multiply(1-true,1/(1-pred))\n",
        "\n",
        "\n",
        "\n",
        "class act(metaclass=ABCMeta):\n",
        "  '''\n",
        "  The abstract base class for activation function.\n",
        "  For each loss,\n",
        "  the functions used for forward and backward propagation are specified respectively.\n",
        "  The two functions take same inputs.\n",
        "  The forward function would return the values after the transformation.\n",
        "  The backward function would return the derivative musk at this layer.\n",
        "  '''\n",
        "  def forward(self,matrix):\n",
        "    pass\n",
        "  \n",
        "  def backward(self,matrix):\n",
        "    pass\n",
        "\n",
        "\n",
        "class linear(act):\n",
        "  '''Linear activation function.'''\n",
        "  def forward(self,matrix):\n",
        "    return matrix\n",
        "  \n",
        "  def backward(self,matrix):\n",
        "    return np.ones_like(matrix)\n",
        "\n",
        "class relu(act):\n",
        "  '''Rectified linear units.'''\n",
        "  def forward(self,matrix):\n",
        "    return  np.multiply(matrix>0,matrix)\n",
        "  \n",
        "  def backward(self,matrix):\n",
        "    return 1*(matrix>0)\n",
        "  \n",
        "\n",
        "class logistic(act):\n",
        "  '''Logistic transformation'''\n",
        "  def forward(self,matrix):\n",
        "    return 1/(1+np.exp(-matrix) + 0.000001)\n",
        "  \n",
        "  def backward(self,matrix):# 逻辑回归导数  g(x)(1-g(x))\n",
        "    return np.multiply(self.forward(matrix),1-self.forward(matrix))\n",
        "\n",
        "# \n",
        "# 1个epoch表示过了1遍训练集中的所有样本\n",
        "# batch-size：1次迭代所使用的样本量\n",
        "# iteration：表示1次迭代,每次迭代更新1次网络结构的参数\n",
        "# step 训练模型步数\n",
        "# 表示每运行一个iteration/step，更新一次参数权重，即进行一次学习，每一次更新参数需要batch size个样本进行运算学习，根据运算结果调整更新一次参数。\n",
        "\n",
        "# iteration = sample_num*epoch/batch_size\n",
        "\n",
        "class MLP(object):\n",
        "  '''\n",
        "  Parameters:\n",
        "  ----------\n",
        "  n_hidden_units:Number of units in the hidden layer.\n",
        "  batch_size: Number of data points used in each gradient step(每一梯度步).\n",
        "  n_epochs: Number of epochs.\n",
        "            Note that this determines the number of epochs (how many times each data\n",
        "            point will be used),not the number of gradient steps.\n",
        "  learning_rate: The learning rate of gradient descent(梯度下降).\n",
        "  momentum:  Momentum for gradient descent update. (Between 0 and 1)(梯度下降更新的动量)\n",
        "  weight_decay: Coeffecients for L2 regularization. (Also known as weight decay.)(权重衰减)\n",
        "  activation: Activation function for the hidden layer.\n",
        "              'relu' for rectified linear units.(整流线性单位函数)\n",
        "              'logistic' for sigmoid activation.\n",
        "              'linear' for linear activation.\n",
        "  loss: loss function.\n",
        "      'mse' for regression task.\n",
        "      'log_loss' for classfication task.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               n_hidden_units=100,\n",
        "               batch_size=200,\n",
        "               n_epochs=200,\n",
        "               learning_rate=0.01,\n",
        "               momentum=0.9,\n",
        "               weight_decay=0.0001,\n",
        "               activation='relu',\n",
        "               loss='mse'):\n",
        "    self.n_hidden_units=n_hidden_units\n",
        "    self.batch_size=batch_size\n",
        "    self.n_epochs=n_epochs\n",
        "    self.learning_rate=learning_rate\n",
        "    self.momentum=momentum\n",
        "    self.weight_decay=weight_decay\n",
        "\n",
        "    # activation (This is the activation function for the hidden layer.)\n",
        "    if activation=='relu':\n",
        "      self.act1=relu()\n",
        "    elif activation=='logistic':\n",
        "      self.act1=logistic()\n",
        "    elif activation=='linear':\n",
        "      self.act1=linear()\n",
        "    else:\n",
        "      self.act1=activaion\n",
        "    \n",
        "    # loss (Note that the activation function for the output layer is determined by the loss.)\n",
        "    if loss=='mse':\n",
        "      self.loss=mse()\n",
        "      self.act2=linear()\n",
        "    elif loss=='logistic':\n",
        "      self.loss=log_loss()\n",
        "      self.act2=logistic()\n",
        "    else:\n",
        "      self.loss=loss[0]\n",
        "      self.act2=loss[1]\n",
        "\n",
        "  def forward(self):\n",
        "    self.layer1=self.W1*self.X + self.b1\n",
        "    self.layer1act=self.act1.forward(self.layer1)\n",
        "    self.score=self.W2*self.layer1act+self.b2\n",
        "    self.pred=self.act2.forward(self.socre)\n",
        "  \n",
        "  def backward(self):\n",
        "    self.dpred=self.loss.gradient(self.pred,self.true)\n",
        "    self.dscore=np.multiply(self.dpred,self.act2.backward(self.score))\n",
        "    self.dlayer1act=self.W2.T*self.dscore\n",
        "    self.dlayer1=np.multiply(self.dlayer1act,self.act1.backward(self.layer1))\n",
        "\n",
        "    self.dW1=(self.dlayer1*self.X.T - self.weight_decay*self.W1)/self.batch_size\n",
        "    self.db1=np.sum(self.dlayer1,axis=1)/self.batch_size\n",
        "    self.dW2=(self.dscore*self.layer1act.T - self.weight_decay*self.W2)/self.batch_size\n",
        "    self.db2=np.sum(self.dscore,axis=1)/self.batch_size\n",
        "\n",
        "  def update_weights(self):\n",
        "    # calculate moving average gradients (momentum)\n",
        "    self.tW1 = self.momentum*self.tW1 + (1-self.momentum)*self.dW1\n",
        "    self.tb1 = self.momentum*self.tb1 + (1-self.momentum)*self.db1\n",
        "    self.tW2 = self.momentum*self.tW2 + (1-self.momentum)*self.dW2\n",
        "    self.tb2 = self.momentum*self.tb2 + (1-self.momentum)*self.db2\n",
        "\n",
        "    # update weights\n",
        "    self.W1 -= self.tW1*self.learning_rate\n",
        "    self.b1 -= self.tb1*self.learning_rate\n",
        "    self.W2 -= self.tW2*self.learning_rate\n",
        "    self.b2 -= self.tb2*self.learning_rate\n",
        "  \n",
        "  def fit(self,train,target):\n",
        "    # turn the inputs into matrices.\n",
        "    train=np.matrix(train).T\n",
        "    target=np.matrix(target.reshape(-1,1)).T\n",
        "\n",
        "    # parameters\n",
        "    n_features=train.shape[0]\n",
        "    n_obs=train.shape[1]\n",
        "\n",
        "    # weight initialization\n",
        "    s1=np.sqrt(6/n_features + self.n_hidden_units)\n",
        "    s2=np.sart(6/(1+self.n_hidden_units))\n",
        "    self.W1=np.matrix(np.random.uniform(-s1,s1,[self.n_hidden_units,n_features]))\n",
        "    self.b1=np.matrix(np.random.uniform(-s1,s1,[self.n_hidden_units,1]))\n",
        "    self.W2=np.matrix(np.random.uniform(-s2,s2,[1,self.n_hidden_units]))\n",
        "    self.b2=np.matrix(np.random.uniform(-s2,s2,[1,1]))\n",
        "\n",
        "    # momentum initialization\n",
        "    self.tW1=self.W1*0\n",
        "    self.tb1=self.b1*0\n",
        "    self.tW2=self.W2*0\n",
        "    self.tb2=self.b2*0\n",
        "\n",
        "    # the training process\n",
        "    for i in range(self.n_epochs):\n",
        "      for j in range(n_obs/self.batch_size):\n",
        "        self.X=train[:,j*self.batch_size:(j+1)*self.batch_size]\n",
        "        self.true=target[:,j*self.batch_size:(j+1)*self.batch_size]\n",
        "        self.forward()\n",
        "        self.backward()\n",
        "        self.update_weights()\n",
        "\n",
        "def predict(self,test):\n",
        "  self.X=np.matrix(test).T\n",
        "  self.forward()\n",
        "  return np.squeeze(np.asarray(self.pred))\n",
        "\n",
        "# np.squeeze() 函数可以删除数组形状中的单维度条目，即把shape中为1的维度去掉，但是对非单维的维度不起作用。\n",
        "# np.asarray() 将结构数据转化为ndarray。\n"
      ],
      "metadata": {
        "id": "cmjUv21ep8q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ihFEWws-zIhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}